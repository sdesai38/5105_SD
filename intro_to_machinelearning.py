# -*- coding: utf-8 -*-
"""Intro_to_MachineLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CA52TgI369adWgm2juAAKmto8Be2gNzo

# Homework 1

Importing the needed libraries, renaming ailiases, and mounting drive to later import data set
"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

"""Import the data set
Remember x0 = 1 always, implied
"""

filepath = '/content/drive/My Drive/IntroToMLDataSets/D3.csv'
dataset = pd.DataFrame(pd.read_csv(filepath))
dataset.head()

#Seperate the x and y arrays
X = dataset.values[:, [0,1,2]]
Y = dataset.values[:, 3]
m = len(Y)
n = len(X)

print('X = ', X[: 5])
print('Y = ', Y[: 5])
print('m = ', m)
print('n = ', n)

print(X)

from IPython.display import display
display(dataset)

plt.scatter(X[:,0], Y, color= 'red', marker='+')

#Grid, Labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6,6)
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Scatter plot of X1 Vs Y')

plt.show()



plt.scatter(X[:,1], Y, color= 'red', marker='+')

#Grid, Labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6,6)
plt.xlabel('X2')
plt.ylabel('Y')
plt.title('Scatter plot of X2 Vs Y')

plt.show()

plt.scatter(X[:,2], Y, color= 'red', marker='+')

#Grid, Labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6,6)
plt.xlabel('X3')
plt.ylabel('Y')
plt.title('Scatter plot of X3 Vs Y')

plt.show()

plt.scatter(X[:,0], Y, color= 'red', marker='+')
plt.scatter(X[:,1], Y, color= 'blue', marker='+')
plt.scatter(X[:,2], Y, color= 'green', marker='+')

#Grid, Labels, and title
plt.grid(True)
plt.rcParams["figure.figsize"] = (6,6)
plt.xlabel('X1,X2,X3')
plt.ylabel('Y')
plt.title('Scatter plot of X1,X2,X3 Vs Y')

plt.show()

X_0 = np.ones((m,1))
X_0[:5]

X_1 = X.reshape(m,3)
X_1[:10]

X = np.hstack((X_0, X_1))
X[:10]

theta = np.zeros(2)
X1 = X[:,[0,1]]
X2 = X[:,[0,2]]
X3 = X[:,[0,3]]
theta

theta1 = [0.,0.]
theta2 = [0.,0.]
theta3 = [0.,0.]
iterations = 1500
alpha = 0.001
theta

def compute_cost(X, Y, theta):
  """
  Parameters(for the single variable calculation):
  X: 2D array where each row reperesent the training example and each column represent the



  """
  predictions = X.dot(theta)
  errors = np.subtract(predictions, Y)
  sqrErrors = np.square(errors)
  J = (1/(2*m)) * np.sum(sqrErrors)
  return J



cost = compute_cost(X1, Y, theta1)
print('The cost for the given values of theta_0 and theta_1 =',cost)

def gradient_descent(X, Y, theta, alpha, iterations):

  m = len(Y)
  cost_history = np.zeros(iterations)

  for i in range(iterations):
    predictions = X.dot(theta)
    errors = np.subtract(predictions,Y)
    sum_delta = (alpha / m) * X.transpose().dot(errors)
    theta -= sum_delta
    cost_history[i] = compute_cost(X, Y, theta)

  return theta, cost_history

"""Part 1) Just X1"""

theta1, cost_history = gradient_descent(X1,Y,theta1,alpha,iterations)
print('Final value of theta =', theta1)
print('cost_history = ', cost_history)

plt.scatter(X1[:,1], Y, color='red',marker='+', label='Training Data')

plt.plot(X1[:,1],X1.dot(theta1),color = 'green', label='Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid(True)
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Linear Regression Fit')
plt.legend()

plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

# Show the plot
plt.show()

"""just X2"""

theta2, cost_history = gradient_descent(X2,Y,theta2,alpha,iterations)
print('Final value of theta =', theta2)
print('cost_history = ', cost_history)

plt.scatter(X2[:,1], Y, color='red',marker='+', label='Training Data')

plt.plot(X2[:,1],X2.dot(theta2),color = 'green', label='Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid(True)
plt.xlabel('X2')
plt.ylabel('Y')
plt.title('Linear Regression Fit')
plt.legend()

plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

# Show the plot
plt.show()

"""Just X3"""

theta3, cost_history = gradient_descent(X3,Y,theta3,alpha,iterations)
print('Final value of theta =', theta3)
print('cost_history = ', cost_history)

plt.scatter(X3[:,1], Y, color='red',marker='+', label='Training Data')

plt.plot(X3[:,1],X3.dot(theta3),color = 'green', label='Linear Regression')

plt.rcParams["figure.figsize"] = (10,6)
plt.grid(True)
plt.xlabel('X3')
plt.ylabel('Y')
plt.title('Linear Regression Fit')
plt.legend()

plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

# Show the plot
plt.show()

"""3. Explanatory variables X2 has the lowest cost across multiple lerning rate variations. (.1, .01, .001)

4. Different learning rates changed the linear regression model for each of the indepenent X's and Y. Lowering the value to .001 changed the way the convergence of gradient decent and took a longer number of iterations for convergence. incfeasing the value to .1 actually overfit the X3vsY graph but took significantly less iterations for convergence on all 3

Multi-Variable Part B)
"""

theta = [0.,0.,0.,0.]
alpha = .1

X = np.hstack((X_0, X_1))

theta, cost_history = gradient_descent(X,Y,theta,alpha,iterations)
print('Final value of theta =', theta)
print('cost_history = ', cost_history)

cost = compute_cost(X, Y, theta)
print('The cost for the given values of theta_0 and theta_1 =',cost)

# Choose two features to visualize
x1_range = np.linspace(X[:,1].min(), X[:,1].max(), 10)
x2_range = np.linspace(X[:,2].min(), X[:,2].max(), 10)
x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)

# Fix X3 at its mean
x3_fixed = np.mean(X[:,3])
y_pred_grid = theta[0] + theta[1]*x1_grid + theta[2]*x2_grid + theta[3]*x3_fixed

#plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,1], X[:,2], Y, color='blue', label='Data')
ax.plot_surface(x1_grid, x2_grid, y_pred_grid, color='orange', alpha=0.5, label='Regression Plane')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Y')
plt.show()

# Choose two features to visualize
x1_range = np.linspace(X[:,1].min(), X[:,1].max(), 10)
x3_range = np.linspace(X[:,3].min(), X[:,3].max(), 10)
x1_grid, x3_grid = np.meshgrid(x1_range, x3_range)

# Fix X2 at its mean
x2_fixed = np.mean(X[:,2])
y_pred_grid = theta[0] + theta[1]*x1_grid + theta[2]*x2_fixed + theta[3]*x3_grid

#plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,1], X[:,3], Y, color='blue', label='Data')
ax.plot_surface(x1_grid, x2_grid, y_pred_grid, color='orange', alpha=0.5, label='Regression Plane')
ax.set_xlabel('X1')
ax.set_ylabel('X3')
ax.set_zlabel('Y')
plt.show()

# Choose two features to visualize
x2_range = np.linspace(X[:,2].min(), X[:,2].max(), 10)
x3_range = np.linspace(X[:,3].min(), X[:,3].max(), 10)
x2_grid, x3_grid = np.meshgrid(x2_range, x3_range)

# Fix X1 at its mean
x1_fixed = np.mean(X[:,1])
y_pred_grid = theta[0] + theta[1]*x1_fixed + theta[2]*x2_grid + theta[3]*x3_grid

#plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,2], X[:,3], Y, color='blue', label='Data')
ax.plot_surface(x1_grid, x2_grid, y_pred_grid, color='orange', alpha=0.5, label='Regression Plane')
ax.set_xlabel('X2')
ax.set_ylabel('X3')
ax.set_zlabel('Y')
plt.show()

plt.plot(range(1, iterations + 1), cost_history, color='blue')
plt.rcParams["figure.figsize"] = (10, 6)
plt.grid(True)

plt.xlabel('Number of iterations')
plt.ylabel('Cost (J)')
plt.title('Convergence of gradient descent')

# Show the plot
plt.show()

Ypred = theta[0] + theta[1]*X[:,1] + theta[2]*X[:,2] + theta[3]*X[:,3]
plt.scatter(Y, Ypred, color="blue", alpha=0.6, label="Data")
plt.plot([min(Y), max(Y)], [min(Y), max(Y)], "r--", label="Ideal fit")
plt.xlabel("Actual Y")
plt.ylabel("Predicted Y")
plt.title("Predicted vs Actual")
plt.legend()
plt.show()

"""3. The different learning rates impact the cost function as well as the linear regression plane. For example in X2,X3 vs Y with X1 mean as fixed, showed a thicker linear regression plane at alpha = 0.1 than when tested at 0.01

4.Predict the value of y for new (X1, X2, X3) values (1, 1, 1), for (2, 0, 4), and for (3, 2, 1)


Below are the equations, the values are: 3.577, 0.244 0.103
"""

Y = theta[0] + theta[1]*1 + theta[2]*1 + theta[3]*1
print(Y)

Y = theta[0] + theta[1]*2 + theta[2]*0 + theta[3]*4
print(Y)

Y = theta[0] + theta[1]*3 + theta[2]*2 + theta[3]*1
print(Y)